{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This project has a few steps:\n",
    "First, we will use 4 feature selection methods: RF feature importance, XGB feature importance, Fisher score, and Chi-squared score to select the top 15 features.\n",
    "Then, we will use 4 models: Logistic Regression, Random Forest, XGBoost, and Neural Network to train the models with hyperparameter tuning.\n",
    "Later, we will perform bootstrapping for each chosen model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skfeature.function.similarity_based import fisher_score\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the data\n",
    "current_dir = os.getcwd()\n",
    "csv_file = f'{current_dir}/spotify-2023.csv'\n",
    "try:\n",
    "    df = pd.read_csv(csv_file, encoding='utf-8-sig')\n",
    "except UnicodeDecodeError:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, encoding='latin1')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(csv_file, encoding='ISO-8859-1')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "# Drop rows with NaN values in 'streams' column\n",
    "# df.dropna(subset=['streams'], inplace=True) # there aren't any NaN values in the 'streams' column\n",
    "\n",
    "# replace all NaN values in the 'key' column with 'Unknown'\n",
    "df['key'] = df['key'].fillna('Unknown')\n",
    "\n",
    "# replace all Nan values in the 'in_shazam_charts' column with 0\n",
    "df['in_shazam_charts'] = df['in_shazam_charts'].fillna(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "# find the median of the 'streams' column\n",
    "median_streams = df['streams'].median()\n",
    "\n",
    "# change the 'streams' column to categorical- 1 if the number of streams is greater than the median, 0 otherwise\n",
    "df['streams'] = df['streams'].apply(lambda x: 1 if x > median_streams else 0)\n",
    "\n",
    "# Split the data into features and target\n",
    "y = df['streams']\n",
    "X = df.drop(columns=['streams', 'track_name', 'artist(s)_name']) # removing the target and the IDs\n",
    "\n",
    "# convert categorical columns to numerical columns\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.21, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "# Use a random forest classifier for feature selection\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances_rf = rf.feature_importances_\n",
    "\n",
    "# Create a dataframe of the feature importances\n",
    "features = X_train.columns\n",
    "feature_importances_rf_df = pd.DataFrame({'feature': features, 'importance': feature_importances_rf})\n",
    "\n",
    "# Sort the dataframe by feature importances\n",
    "feature_importances_rf_df = feature_importances_rf_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Create a barplot of the feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importances_rf_df)\n",
    "plt.title('Feature Importances- Random Forest')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use XGBoost for feature selection\n",
    "# change the target column to numerical\n",
    "y_train_XGB = y_train\n",
    "y_test_XGB = y_test\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "xgb = XGBClassifier(n_estimators=100, random_state=42)\n",
    "xgb.fit(X_train, y_train_XGB)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = xgb.feature_importances_\n",
    "\n",
    "# Create a dataframe of the feature importances\n",
    "features = X_train.columns\n",
    "feature_importances_df = pd.DataFrame({'feature': features, 'importance': feature_importances})\n",
    "\n",
    "# Sort the dataframe by feature importances\n",
    "feature_importances_df = feature_importances_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Create a barplot of the feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importances_df)\n",
    "plt.title('Feature Importances- XGBoost')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use fisher score for feature selection\n",
    "# Get the feature scores\n",
    "feature_fisher_scores = fisher_score.fisher_score(X_train.values, y_train.values)\n",
    "\n",
    "# Create a dataframe of the feature scores\n",
    "feature_fisher_scores_df = pd.DataFrame({'feature': features, 'score': feature_fisher_scores})\n",
    "\n",
    "# Sort the dataframe by feature scores\n",
    "feature_fisher_scores_df = feature_fisher_scores_df.sort_values(by='score', ascending=False)\n",
    "\n",
    "# Create a barplot of the feature scores\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='score', y='feature', data=feature_fisher_scores_df)\n",
    "plt.title('Fisher Scores')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use chi-squared score for feature selection\n",
    "# Get the p-values\n",
    "p_values = chi2(X_train, y_train)[1]\n",
    "# plot the p-values\n",
    "p_values_df = pd.DataFrame({'feature': features, 'p_value': p_values})\n",
    "p_values_df = p_values_df.sort_values(by='p_value', ascending=True)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='p_value', y='feature', data=p_values_df)\n",
    "plt.title('Chi-squared p-values')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use chi-squared score for feature selection\n",
    "# Get the feature scores\n",
    "feature_scores = chi2(X_train, y_train)[0]\n",
    "\n",
    "# Create a dataframe of the feature scores\n",
    "feature_scores_df = pd.DataFrame({'feature': features, 'score': feature_scores})\n",
    "\n",
    "# Sort the dataframe by feature scores\n",
    "feature_scores_df = feature_scores_df.sort_values(by='score', ascending=False)\n",
    "\n",
    "# show the features that have p-values less than 0.05\n",
    "p_values = chi2(X_train, y_train)[1]\n",
    "p_values_df = pd.DataFrame({'feature': features, 'p_value': p_values})\n",
    "\n",
    "p_values_df = p_values_df.sort_values(by='p_value', ascending=True)\n",
    "# print(p_values_df)\n",
    "\n",
    "p_values_df = p_values_df[p_values_df['p_value'] < 0.05]\n",
    "print(p_values_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the top 15 features from each feature selection method\n",
    "# Save the top 15 features from the random forest feature importances\n",
    "top_15_rf_features = feature_importances_rf_df['feature'][:15]\n",
    "top_15_rf_features.to_csv('top_15_rf_features.csv', index=False)\n",
    "rf_features_test = X_test[top_15_rf_features]\n",
    "\n",
    "# Save the top 15 features from the XGBoost feature importances\n",
    "top_15_xgb_features = feature_importances_df['feature'][:15]\n",
    "top_15_xgb_features.to_csv('top_15_xgb_features.csv', index=False)\n",
    "xgb_features_test = X_test[top_15_xgb_features]\n",
    "\n",
    "# Save the top 15 features from the fisher scores\n",
    "top_15_fisher_features = feature_fisher_scores_df['feature'][:15]\n",
    "top_15_fisher_features.to_csv('top_15_fisher_features.csv', index=False)\n",
    "fisher_features_test = X_test[top_15_fisher_features]\n",
    "\n",
    "# Save all features that have p-values less than 0.05\n",
    "p_values_df.to_csv('p_values.csv', index=False)\n",
    "chi2_features_test = X_test[p_values_df['feature']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the models\n",
    "# Logistic Regression\n",
    "# Create a logistic regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "log_reg_rf = LogisticRegression(random_state=42)\n",
    "log_reg_grid = {'C': [0.000001, 0.00001, 0.0001, 0.001, 0.01, 1], 'max_iter': [100, 200]}\n",
    "log_reg_rf_Grid = GridSearchCV(log_reg_rf, log_reg_grid, cv=5, verbose=2, scoring='accuracy')\n",
    "log_reg_rf_features_grid = log_reg_rf_Grid.fit(X_train[top_15_rf_features], y_train)\n",
    "\n",
    "log_reg_XGB = LogisticRegression(random_state=42)\n",
    "log_reg_grid = {'C': [0.000001, 0.00001, 0.0001, 0.001, 0.01, 1], 'max_iter': [100, 200]}\n",
    "log_reg_XGB_Grid = GridSearchCV(log_reg_XGB, log_reg_grid, cv=5, verbose=2, scoring='accuracy')\n",
    "log_reg_XGB_features_grid = log_reg_XGB_Grid.fit(X_train[top_15_xgb_features], y_train)\n",
    "\n",
    "log_reg_Fisher = LogisticRegression(random_state=42)\n",
    "log_reg_grid = {'C': [0.000001, 0.00001, 0.0001, 0.001, 0.01, 1], 'max_iter': [100, 200]}\n",
    "log_reg_Fisher_Grid = GridSearchCV(log_reg_Fisher, log_reg_grid, cv=5, verbose=2, scoring='accuracy')\n",
    "log_reg_Fisher_features_grid = log_reg_Fisher_Grid.fit(X_train[top_15_fisher_features], y_train)\n",
    "\n",
    "log_reg_Chi2 = LogisticRegression(random_state=42)\n",
    "log_reg_grid = {'C': [0.000001, 0.00001, 0.0001, 0.001, 0.01, 1], 'max_iter': [100, 200]}\n",
    "log_reg_Chi2_Grid = GridSearchCV(log_reg_Chi2, log_reg_grid, cv=5, verbose=2, scoring='accuracy')\n",
    "log_reg_Chi2_features_grid = log_reg_Chi2_Grid.fit(X_train[p_values_df['feature']], y_train)\n",
    "\n",
    "# Save the best hyperparameters for each feature selection method\n",
    "best_hyperparameters = {'rf_features': log_reg_rf_features_grid.best_params_, 'XGB_features': log_reg_XGB_features_grid.best_params_,\n",
    "                        'Fisher_features': log_reg_Fisher_features_grid.best_params_, 'Chi2_features': log_reg_Chi2_features_grid.best_params_}\n",
    "best_hyperparameters_df = pd.DataFrame(best_hyperparameters, index=[0])\n",
    "best_hyperparameters_df.to_csv('best_hyperparameters.csv', index=False)\n",
    "\n",
    "# save the best model for each feature selection method\n",
    "log_reg_rf_features = log_reg_rf_features_grid.best_estimator_\n",
    "log_reg_XGB_features = log_reg_XGB_features_grid.best_estimator_\n",
    "log_reg_Fisher_features = log_reg_Fisher_features_grid.best_estimator_\n",
    "log_reg_Chi2_features = log_reg_Chi2_features_grid.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print the best hyperparameters for each feature selection method\n",
    "print(f'Logistic Regression with RF features: {log_reg_rf_features}')\n",
    "print(f'Logistic Regression with XGB features: {log_reg_XGB_features}')\n",
    "print(f'Logistic Regression with Fisher features: {log_reg_Fisher_features}')\n",
    "print(f'Logistic Regression with Chi2 features: {log_reg_Chi2_features}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# Create a random forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf_rf = RandomForestClassifier(random_state=42)\n",
    "rf_grid = {'n_estimators': [100, 300, 500], 'max_depth': [10, 20, 30, 40, None], 'min_samples_leaf': [1, 2, 4]}\n",
    "rf_rf_Grid = GridSearchCV(rf_rf, rf_grid, cv=5, verbose=2, scoring='accuracy')\n",
    "rf_rf_features_grid = rf_rf_Grid.fit(X_train[top_15_rf_features], y_train)\n",
    "\n",
    "rf_XGB = RandomForestClassifier(random_state=42)\n",
    "rf_grid = {'n_estimators': [100, 300, 500], 'max_depth': [10, 20, 30, 40, None], 'min_samples_leaf': [1, 2, 4]}\n",
    "rf_XGB_Grid = GridSearchCV(rf_XGB, rf_grid, cv=5, verbose=2, scoring='accuracy')\n",
    "rf_XGB_features_grid = rf_XGB_Grid.fit(X_train[top_15_xgb_features], y_train)\n",
    "\n",
    "rf_Fisher = RandomForestClassifier(random_state=42)\n",
    "rf_grid = {'n_estimators': [100, 300, 500], 'max_depth': [10, 20, 30, 40, None], 'min_samples_leaf': [1, 2, 4]}\n",
    "rf_Fisher_Grid = GridSearchCV(rf_Fisher, rf_grid, cv=5, verbose=2, scoring='accuracy')\n",
    "rf_Fisher_features_grid = rf_Fisher_Grid.fit(X_train[top_15_fisher_features], y_train)\n",
    "\n",
    "rf_Chi2 = RandomForestClassifier(random_state=42)\n",
    "rf_grid = {'n_estimators': [100, 300, 500], 'max_depth': [10, 20, 30, 40, None], 'min_samples_leaf': [1, 2, 4]}\n",
    "rf_Chi2_Grid = GridSearchCV(rf_Chi2, rf_grid, cv=5, verbose=2, scoring='accuracy')\n",
    "rf_Chi2_features_grid = rf_Chi2_Grid.fit(X_train[p_values_df['feature']], y_train)\n",
    "\n",
    "# Save the best hyperparameters for each feature selection method\n",
    "best_hyperparameters = {'rf_features': rf_rf_features_grid.best_params_, 'XGB_features': rf_XGB_features_grid.best_params_,\n",
    "                        'Fisher_features': rf_Fisher_features_grid.best_params_, 'Chi2_features': rf_Chi2_features_grid.best_params_}\n",
    "best_hyperparameters_df = pd.DataFrame(best_hyperparameters, index=[0])\n",
    "best_hyperparameters_df.to_csv('best_hyperparameters.csv', index=False)\n",
    "\n",
    "# save the best model for each feature selection method\n",
    "rf_rf_features = rf_rf_features_grid.best_estimator_\n",
    "rf_XGB_features = rf_XGB_features_grid.best_estimator_\n",
    "rf_Fisher_features = rf_Fisher_features_grid.best_estimator_\n",
    "rf_Chi2_features = rf_Chi2_features_grid.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print the best hyperparameters for each feature selection method\n",
    "print(f'Random Forest with RF features: {rf_rf_features}')\n",
    "print(f'Random Forest with XGB features: {rf_XGB_features}')\n",
    "print(f'Random Forest with Fisher features: {rf_Fisher_features}')\n",
    "print(f'Random Forest with Chi2 features: {rf_Chi2_features}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# combine the chosen features in a dataframe\n",
    "chosen_features = pd.concat([top_15_rf_features, top_15_xgb_features, top_15_fisher_features, p_values_df['feature']], axis=1)\n",
    "chosen_features.columns = ['rf_features', 'XGB_features', 'Fisher_features', 'Chi2_features']\n",
    "chosen_features.to_csv('chosen_features.csv', index=False)\n",
    "print(chosen_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "# Create an XGBoost model\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "xgb_rf = XGBClassifier(random_state=42)\n",
    "xgb_grid = {'n_estimators': [100, 300, 500], 'max_depth': [3, 5, 7, 9],\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.2]}\n",
    "xgb_rf_Grid = GridSearchCV(xgb_rf, xgb_grid, cv=5, verbose=2, scoring='accuracy')\n",
    "xgb_rf_features_grid = xgb_rf_Grid.fit(X_train[top_15_rf_features], y_train_XGB)\n",
    "\n",
    "xgb_XGB = XGBClassifier(random_state=42)\n",
    "xgb_grid = {'n_estimators': [100, 300, 500], 'max_depth': [3, 5, 7, 9],\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.2]}\n",
    "xgb_XGB_Grid = GridSearchCV(xgb_XGB, xgb_grid, cv=5, verbose=2, scoring='accuracy')\n",
    "xgb_XGB_features_grid = xgb_XGB_Grid.fit(X_train[top_15_xgb_features], y_train_XGB)\n",
    "\n",
    "xgb_Fisher = XGBClassifier(random_state=42)\n",
    "xgb_grid = {'n_estimators': [100, 300, 500], 'max_depth': [3, 5, 7, 9],\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.2]}\n",
    "xgb_Fisher_Grid = GridSearchCV(xgb_Fisher, xgb_grid, cv=5, verbose=2, scoring='accuracy')\n",
    "xgb_Fisher_features_grid = xgb_Fisher_Grid.fit(X_train[top_15_fisher_features], y_train_XGB)\n",
    "\n",
    "xgb_Chi2 = XGBClassifier(random_state=42)\n",
    "xgb_grid = {'n_estimators': [100, 300, 500], 'max_depth': [3, 5, 7, 9],\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.2]}\n",
    "xgb_Chi2_Grid = GridSearchCV(xgb_Chi2, xgb_grid, cv=5, verbose=2, scoring='accuracy')\n",
    "xgb_Chi2_features_grid = xgb_Chi2_Grid.fit(X_train[p_values_df['feature']], y_train_XGB)\n",
    "\n",
    "# Save the best hyperparameters for each feature selection method\n",
    "best_hyperparameters = {'rf_features': xgb_rf_features_grid.best_params_, 'XGB_features': xgb_XGB_features_grid.best_params_,\n",
    "                        'Fisher_features': xgb_Fisher_features_grid.best_params_, 'Chi2_features': xgb_Chi2_features_grid.best_params_}\n",
    "best_hyperparameters_df = pd.DataFrame(best_hyperparameters, index=[0])\n",
    "best_hyperparameters_df.to_csv('best_hyperparameters.csv', index=False)\n",
    "\n",
    "# save the best model for each feature selection method\n",
    "xgb_rf_features = xgb_rf_features_grid.best_estimator_\n",
    "xgb_XGB_features = xgb_XGB_features_grid.best_estimator_\n",
    "xgb_Fisher_features = xgb_Fisher_features_grid.best_estimator_\n",
    "xgb_Chi2_features = xgb_Chi2_features_grid.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print the best hyperparameters for each feature selection method\n",
    "print(f'XGBoost with RF features: {xgb_rf_features}')\n",
    "print(f'XGBoost with XGB features: {xgb_XGB_features}')\n",
    "print(f'XGBoost with Fisher features: {xgb_Fisher_features}')\n",
    "print(f'XGBoost with Chi2 features: {xgb_Chi2_features}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "# Create a neural network model\n",
    "nn = MLPClassifier(random_state=42)\n",
    "\n",
    "nn_rf = MLPClassifier(random_state=42)\n",
    "nn_grid = {'hidden_layer_sizes': [(50, 50, 50), (50, 100, 50), (100,), (20, 20, 20, 20)], 'activation': ['tanh', 'relu'],\n",
    "           'solver': ['sgd', 'adam'], 'alpha': [0.001, 0.05]}\n",
    "nn_rf_Grid = GridSearchCV(nn_rf, nn_grid, cv=5, verbose=2, scoring='accuracy')\n",
    "nn_rf_features_grid = nn_rf_Grid.fit(X_train[top_15_rf_features], y_train)\n",
    "\n",
    "nn_XGB = MLPClassifier(random_state=42)\n",
    "nn_grid = {'hidden_layer_sizes': [(50, 50, 50), (50, 100, 50), (100,), (20, 20, 20, 20)], 'activation': ['tanh', 'relu'],\n",
    "           'solver': ['sgd', 'adam'], 'alpha': [0.001, 0.05]}\n",
    "nn_XGB_Grid = GridSearchCV(nn_XGB, nn_grid, cv=5, verbose=2, scoring='accuracy')\n",
    "nn_XGB_features_grid = nn_XGB_Grid.fit(X_train[top_15_xgb_features], y_train)\n",
    "\n",
    "nn_Fisher = MLPClassifier(random_state=42)\n",
    "nn_grid = {'hidden_layer_sizes': [(50, 50, 50), (50, 100, 50), (100,), (20, 20, 20, 20)], 'activation': ['tanh', 'relu'],\n",
    "           'solver': ['sgd', 'adam'], 'alpha': [0.001, 0.05]}\n",
    "nn_Fisher_Grid = GridSearchCV(nn_Fisher, nn_grid, cv=5, verbose=2, scoring='accuracy')\n",
    "nn_Fisher_features_grid = nn_Fisher_Grid.fit(X_train[top_15_fisher_features], y_train)\n",
    "\n",
    "nn_Chi2 = MLPClassifier(random_state=42)\n",
    "nn_grid = {'hidden_layer_sizes': [(50, 50, 50), (50, 100, 50), (100,), (20, 20, 20, 20)], 'activation': ['tanh', 'relu'],\n",
    "           'solver': ['sgd', 'adam'], 'alpha': [0.001, 0.05]}\n",
    "nn_Chi2_Grid = GridSearchCV(nn_Chi2, nn_grid, cv=5, verbose=2, scoring='accuracy')\n",
    "nn_Chi2_features_grid = nn_Chi2_Grid.fit(X_train[p_values_df['feature']], y_train)\n",
    "\n",
    "# Save the best hyperparameters for each feature selection method\n",
    "best_hyperparameters = {'rf_features': nn_rf_features_grid.best_params_, 'XGB_features': nn_XGB_features_grid.best_params_,\n",
    "                        'Fisher_features': nn_Fisher_features_grid.best_params_, 'Chi2_features': nn_Chi2_features_grid.best_params_}\n",
    "best_hyperparameters_df = pd.DataFrame(best_hyperparameters, index=[0])\n",
    "best_hyperparameters_df.to_csv('best_hyperparameters.csv', index=False)\n",
    "\n",
    "# save the best model for each feature selection method\n",
    "nn_rf_features = nn_rf_features_grid.best_estimator_\n",
    "nn_XGB_features = nn_XGB_features_grid.best_estimator_\n",
    "nn_Fisher_features = nn_Fisher_features_grid.best_estimator_\n",
    "nn_Chi2_features = nn_Chi2_features_grid.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print the best hyperparameters for each feature selection method\n",
    "print(f'Neural Network with RF features: {nn_rf_features}')\n",
    "print(f'Neural Network with XGB features: {nn_XGB_features}')\n",
    "print(f'Neural Network with Fisher features: {nn_Fisher_features}')\n",
    "print(f'Neural Network with Chi2 features: {nn_Chi2_features}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing the models to get a sense of their performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use the test set to evaluate the models\n",
    "# Logistic Regression\n",
    "# Evaluate the logistic regression model with the validation set\n",
    "log_reg_rf_features_val_pred = log_reg_rf_features.predict(X_test[top_15_rf_features])\n",
    "log_reg_XGB_features_val_pred = log_reg_XGB_features.predict(X_test[top_15_xgb_features])\n",
    "log_reg_Fisher_features_val_pred = log_reg_Fisher_features.predict(X_test[top_15_fisher_features])\n",
    "log_reg_Chi2_features_val_pred = log_reg_Chi2_features.predict(X_test[p_values_df['feature']])\n",
    "\n",
    "# choose the best model\n",
    "log_reg_rf_features_val_accuracy = accuracy_score(y_test, log_reg_rf_features_val_pred)\n",
    "log_reg_XGB_features_val_accuracy = accuracy_score(y_test, log_reg_XGB_features_val_pred)\n",
    "log_reg_Fisher_features_val_accuracy = accuracy_score(y_test, log_reg_Fisher_features_val_pred)\n",
    "log_reg_Chi2_features_val_accuracy = accuracy_score(y_test, log_reg_Chi2_features_val_pred)\n",
    "\n",
    "log_reg_val_accuracies = {'rf_features': log_reg_rf_features_val_accuracy, 'XGB_features': log_reg_XGB_features_val_accuracy,\n",
    "                          'Fisher_features': log_reg_Fisher_features_val_accuracy, 'Chi2_features': log_reg_Chi2_features_val_accuracy}\n",
    "log_reg_val_accuracies_df = pd.DataFrame(log_reg_val_accuracies, index=[0])\n",
    "log_reg_val_accuracies_df.to_csv('log_reg_val_accuracies.csv', index=False)\n",
    "\n",
    "# print the best model and its feature selection method, and its accuracy\n",
    "log_reg_best_model = max(log_reg_val_accuracies, key=log_reg_val_accuracies.get)\n",
    "print(f'The best logistic regression model is trained with {log_reg_best_model} features and has an accuracy of {log_reg_val_accuracies[log_reg_best_model]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# Evaluate the random forest model with the validation set\n",
    "rf_rf_features_val_pred = rf_rf_features.predict(rf_features_test)\n",
    "rf_XGB_features_val_pred = rf_XGB_features.predict(xgb_features_test)\n",
    "rf_Fisher_features_val_pred = rf_Fisher_features.predict(fisher_features_test)\n",
    "rf_Chi2_features_val_pred = rf_Chi2_features.predict(chi2_features_test)\n",
    "\n",
    "# choose the best model\n",
    "rf_rf_features_val_accuracy = accuracy_score(y_test, rf_rf_features_val_pred)\n",
    "rf_XGB_features_val_accuracy = accuracy_score(y_test, rf_XGB_features_val_pred)\n",
    "rf_Fisher_features_val_accuracy = accuracy_score(y_test, rf_Fisher_features_val_pred)\n",
    "rf_Chi2_features_val_accuracy = accuracy_score(y_test, rf_Chi2_features_val_pred)\n",
    "\n",
    "rf_val_accuracies = {'rf_features': rf_rf_features_val_accuracy, 'XGB_features': rf_XGB_features_val_accuracy,\n",
    "                     'Fisher_features': rf_Fisher_features_val_accuracy, 'Chi2_features': rf_Chi2_features_val_accuracy}\n",
    "rf_val_accuracies_df = pd.DataFrame(rf_val_accuracies, index=[0])\n",
    "rf_val_accuracies_df.to_csv('rf_val_accuracies.csv', index=False)\n",
    "\n",
    "# print the best model and its feature selection method, and its accuracy\n",
    "rf_best_model = max(rf_val_accuracies, key=rf_val_accuracies.get)\n",
    "print(f'The best random forest model is trained with {rf_best_model} features and has an accuracy of {rf_val_accuracies[rf_best_model]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "# Evaluate the XGBoost model with the validation set\n",
    "xgb_rf_features_val_pred = xgb_rf_features.predict(rf_features_test)\n",
    "xgb_XGB_features_val_pred = xgb_XGB_features.predict(xgb_features_test)\n",
    "xgb_Fisher_features_val_pred = xgb_Fisher_features.predict(fisher_features_test)\n",
    "xgb_Chi2_features_val_pred = xgb_Chi2_features.predict(chi2_features_test)\n",
    "\n",
    "# choose the best model\n",
    "xgb_rf_features_val_accuracy = accuracy_score(y_test_XGB, xgb_rf_features_val_pred)\n",
    "xgb_XGB_features_val_accuracy = accuracy_score(y_test_XGB, xgb_XGB_features_val_pred)\n",
    "xgb_Fisher_features_val_accuracy = accuracy_score(y_test_XGB, xgb_Fisher_features_val_pred)\n",
    "xgb_Chi2_features_val_accuracy = accuracy_score(y_test_XGB, xgb_Chi2_features_val_pred)\n",
    "\n",
    "xgb_val_accuracies = {'rf_features': xgb_rf_features_val_accuracy, 'XGB_features': xgb_XGB_features_val_accuracy,\n",
    "                      'Fisher_features': xgb_Fisher_features_val_accuracy, 'Chi2_features': xgb_Chi2_features_val_accuracy}\n",
    "xgb_val_accuracies_df = pd.DataFrame(xgb_val_accuracies, index=[0])\n",
    "xgb_val_accuracies_df.to_csv('xgb_val_accuracies.csv', index=False)\n",
    "\n",
    "# print the best model and its feature selection method, and its accuracy\n",
    "xgb_best_model = max(xgb_val_accuracies, key=xgb_val_accuracies.get)\n",
    "print(f'The best XGBoost model is trained with {xgb_best_model} features and has an accuracy of {xgb_val_accuracies[xgb_best_model]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "# Evaluate the neural network model with the validation set\n",
    "nn_rf_features_val_pred = nn_rf_features.predict(rf_features_test)\n",
    "nn_XGB_features_val_pred = nn_XGB_features.predict(xgb_features_test)\n",
    "nn_Fisher_features_val_pred = nn_Fisher_features.predict(fisher_features_test)\n",
    "nn_Chi2_features_val_pred = nn_Chi2_features.predict(chi2_features_test)\n",
    "\n",
    "# choose the best model\n",
    "nn_rf_features_val_accuracy = accuracy_score(y_test, nn_rf_features_val_pred)\n",
    "nn_XGB_features_val_accuracy = accuracy_score(y_test, nn_XGB_features_val_pred)\n",
    "nn_Fisher_features_val_accuracy = accuracy_score(y_test, nn_Fisher_features_val_pred)\n",
    "nn_Chi2_features_val_accuracy = accuracy_score(y_test, nn_Chi2_features_val_pred)\n",
    "\n",
    "nn_val_accuracies = {'rf_features': nn_rf_features_val_accuracy, 'XGB_features': nn_XGB_features_val_accuracy,\n",
    "                     'Fisher_features': nn_Fisher_features_val_accuracy, 'Chi2_features': nn_Chi2_features_val_accuracy}\n",
    "nn_val_accuracies_df = pd.DataFrame(nn_val_accuracies, index=[0])\n",
    "nn_val_accuracies_df.to_csv('nn_val_accuracies.csv', index=False)\n",
    "\n",
    "# print the best model and its feature selection method, and its accuracy\n",
    "nn_best_model = max(nn_val_accuracies, key=nn_val_accuracies.get)\n",
    "print(f'The best neural network model is trained with {nn_best_model} features and has an accuracy of {nn_val_accuracies[nn_best_model]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bootstrap_accuracy(y_test, pred, num_iterations=1000, confidence_level=0.9):\n",
    "    \"\"\"\n",
    "    Estimate the accuracy of a classification model using bootstrap resampling.\n",
    "\n",
    "    Parameters:\n",
    "        y_test (numpy array): True class labels.\n",
    "        pred (numpy array): Predicted class labels.\n",
    "        num_iterations (int): Number of bootstrap iterations. Default is 1000.\n",
    "        confidence_level (float): Confidence level for the confidence interval. Default is 0.95.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing the point estimate of accuracy and its confidence interval.\n",
    "    \"\"\"\n",
    "    n = len(y_test)\n",
    "    accuracies = []\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        indices = np.random.randint(0, n, n)  # Bootstrap sample indices\n",
    "        y_test_bootstrap = y_test[indices]\n",
    "        pred_bootstrap = pred[indices]\n",
    "\n",
    "        # Calculate accuracy for this bootstrap sample\n",
    "        accuracy = np.mean(y_test_bootstrap == pred_bootstrap)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    # Calculate point estimate of accuracy\n",
    "    point_estimate = np.mean(accuracies)\n",
    "\n",
    "    # Calculate confidence interval\n",
    "    alpha = 1 - confidence_level\n",
    "    lower_percentile = alpha / 2 * 100\n",
    "    upper_percentile = (1 - alpha / 2) * 100\n",
    "    lower_bound = np.percentile(accuracies, lower_percentile)\n",
    "    upper_bound = np.percentile(accuracies, upper_percentile)\n",
    "\n",
    "    return point_estimate, (lower_bound, upper_bound)\n",
    "\n",
    "# Example usage:\n",
    "# convert y_test to a numpy array\n",
    "test = np.array(y_test)\n",
    "pred = log_reg_rf_features_val_pred\n",
    "# Assuming y_test and pred are numpy arrays containing true class labels and predicted class labels respectively\n",
    "accuracy, confidence_interval = bootstrap_accuracy(test, pred)\n",
    "print(\"Point estimate of accuracy:\", accuracy)\n",
    "print(\"Confidence interval:\", confidence_interval)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# perform the bootstrap accuracy for all models\n",
    "y_test = np.array(y_test)\n",
    "# Logistic Regression\n",
    "# Perform bootstrap accuracy estimation for the logistic regression model\n",
    "log_reg_rf_features_accuracy, log_reg_rf_features_confidence_interval = bootstrap_accuracy(y_test, log_reg_rf_features_val_pred)\n",
    "log_reg_XGB_features_accuracy, log_reg_XGB_features_confidence_interval = bootstrap_accuracy(y_test, log_reg_XGB_features_val_pred)\n",
    "log_reg_Fisher_features_accuracy, log_reg_Fisher_features_confidence_interval = bootstrap_accuracy(y_test, log_reg_Fisher_features_val_pred)\n",
    "log_reg_Chi2_features_accuracy, log_reg_Chi2_features_confidence_interval = bootstrap_accuracy(y_test, log_reg_Chi2_features_val_pred)\n",
    "\n",
    "print(f'Logistic Regression with RF features: {log_reg_rf_features_accuracy} ({log_reg_rf_features_confidence_interval})')\n",
    "print(f'Logistic Regression with XGB features: {log_reg_XGB_features_accuracy} ({log_reg_XGB_features_confidence_interval})')\n",
    "print(f'Logistic Regression with Fisher features: {log_reg_Fisher_features_accuracy} ({log_reg_Fisher_features_confidence_interval})')\n",
    "print(f'Logistic Regression with Chi2 features: {log_reg_Chi2_features_accuracy} ({log_reg_Chi2_features_confidence_interval})')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# Perform bootstrap accuracy estimation for the random forest model\n",
    "rf_rf_features_accuracy, rf_rf_features_confidence_interval = bootstrap_accuracy(y_test, rf_rf_features_val_pred)\n",
    "rf_XGB_features_accuracy, rf_XGB_features_confidence_interval = bootstrap_accuracy(y_test, rf_XGB_features_val_pred)\n",
    "rf_Fisher_features_accuracy, rf_Fisher_features_confidence_interval = bootstrap_accuracy(y_test, rf_Fisher_features_val_pred)\n",
    "rf_Chi2_features_accuracy, rf_Chi2_features_confidence_interval = bootstrap_accuracy(y_test, rf_Chi2_features_val_pred)\n",
    "\n",
    "print(f'Random Forest with RF features: {rf_rf_features_accuracy} ({rf_rf_features_confidence_interval})')\n",
    "print(f'Random Forest with XGB features: {rf_XGB_features_accuracy} ({rf_XGB_features_confidence_interval})')\n",
    "print(f'Random Forest with Fisher features: {rf_Fisher_features_accuracy} ({rf_Fisher_features_confidence_interval})')\n",
    "print(f'Random Forest with Chi2 features: {rf_Chi2_features_accuracy} ({rf_Chi2_features_confidence_interval})')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "y_test_XGB = np.array(y_test_XGB)\n",
    "# Perform bootstrap accuracy estimation for the XGBoost model\n",
    "xgb_rf_features_accuracy, xgb_rf_features_confidence_interval = bootstrap_accuracy(y_test_XGB, xgb_rf_features_val_pred)\n",
    "xgb_XGB_features_accuracy, xgb_XGB_features_confidence_interval = bootstrap_accuracy(y_test_XGB, xgb_XGB_features_val_pred)\n",
    "xgb_Fisher_features_accuracy, xgb_Fisher_features_confidence_interval = bootstrap_accuracy(y_test_XGB, xgb_Fisher_features_val_pred)\n",
    "xgb_Chi2_features_accuracy, xgb_Chi2_features_confidence_interval = bootstrap_accuracy(y_test_XGB, xgb_Chi2_features_val_pred)\n",
    "\n",
    "print(f'XGBoost with RF features: {xgb_rf_features_accuracy} ({xgb_rf_features_confidence_interval})')\n",
    "print(f'XGBoost with XGB features: {xgb_XGB_features_accuracy} ({xgb_XGB_features_confidence_interval})')\n",
    "print(f'XGBoost with Fisher features: {xgb_Fisher_features_accuracy} ({xgb_Fisher_features_confidence_interval})')\n",
    "print(f'XGBoost with Chi2 features: {xgb_Chi2_features_accuracy} ({xgb_Chi2_features_confidence_interval})')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "# Perform bootstrap accuracy estimation for the neural network model\n",
    "nn_rf_features_accuracy, nn_rf_features_confidence_interval = bootstrap_accuracy(y_test, nn_rf_features_val_pred)\n",
    "nn_XGB_features_accuracy, nn_XGB_features_confidence_interval = bootstrap_accuracy(y_test, nn_XGB_features_val_pred)\n",
    "nn_Fisher_features_accuracy, nn_Fisher_features_confidence_interval = bootstrap_accuracy(y_test, nn_Fisher_features_val_pred)\n",
    "nn_Chi2_features_accuracy, nn_Chi2_features_confidence_interval = bootstrap_accuracy(y_test, nn_Chi2_features_val_pred)\n",
    "\n",
    "print(f'Neural Network with RF features: {nn_rf_features_accuracy} ({nn_rf_features_confidence_interval})')\n",
    "print(f'Neural Network with XGB features: {nn_XGB_features_accuracy} ({nn_XGB_features_confidence_interval})')\n",
    "print(f'Neural Network with Fisher features: {nn_Fisher_features_accuracy} ({nn_Fisher_features_confidence_interval})')\n",
    "print(f'Neural Network with Chi2 features: {nn_Chi2_features_accuracy} ({nn_Chi2_features_confidence_interval})')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Top 15 features for Random Forest:')\n",
    "print(top_15_rf_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Top 15 features for XGBoost:')\n",
    "print(top_15_xgb_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Top 15 features for Fisher Score:')\n",
    "print(top_15_fisher_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('P-values under 0.05 in Chi-Squared test:')\n",
    "print(p_values_df['feature'])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
